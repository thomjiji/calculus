### 1. Prove that the transpose of the transpose of a matrix is the matrix itself: $(\mathbf{A}^\top)^\top = \mathbf{A}$.

Let's denote the matrix as A. The notation Aᵀ represents the transpose of matrix A.

The transpose of a matrix A, denoted Aᵀ, is obtained by reflecting the matrix over its 
main diagonal (switching rows and columns).

To prove (Aᵀ)ᵀ = A, we need to show that the elements of both sides of the equation 
are equal. That is, for every element at position (i, j) in A, it should be the same as 
the element at position (i, j) in (Aᵀ)ᵀ.

1. Matrix Aᵀ: The transpose of matrix A, denoted Aᵀ, has dimensions n x m.
2. Matrix (Aᵀ)ᵀ: The transpose of matrix Aᵀ, denoted (Aᵀ)ᵀ, has dimensions m x n, which 
is the same as the original matrix A.
3. Element Comparison: For each element (i, j) in A, the element (i, j) in (Aᵀ)ᵀ is the 
same.

Since (Aᵀ)ᵀ and A have the same dimensions and the elements are equal, we can conclude 
that (Aᵀ)ᵀ = A.

### 2. Given two matrices $\mathbf{A}$ and $\mathbf{B}$, show that sum and transposition commute: $\mathbf{A}^\top + \mathbf{B}^\top = (\mathbf{A} + \mathbf{B})^\top$. 

```py
A = torch.tensor([[1, 2, 3], [4, 5, 6], [7, 8, 9]])
B = torch.tensor([[10, 11, 12], [13, 14, 15], [16, 17, 18]])

print("Origin:")
print(A, B, end="\n\n", sep="\n")

print("Transposed:")
print(A.T, B.T, end="\n\n", sep="\n")

print("Transposed before sum:")
print(A.T + B.T, end="\n\n", sep="\n")

print("Transposed after sum:")
print((A + B).T, end="\n\n", sep="\n")
```

It will output:

```
Origin:
tensor([[1, 2, 3],
        [4, 5, 6],
        [7, 8, 9]])
tensor([[10, 11, 12],
        [13, 14, 15],
        [16, 17, 18]])

Transposed:
tensor([[1, 4, 7],
        [2, 5, 8],
        [3, 6, 9]])
tensor([[10, 13, 16],
        [11, 14, 17],
        [12, 15, 18]])

Transposed before sum:
tensor([[11, 17, 23],
        [13, 19, 25],
        [15, 21, 27]])

Transposed after sum:
tensor([[11, 17, 23],
        [13, 19, 25],
        [15, 21, 27]])
```

As you can see, the result of Aᵀ + Bᵀ and (A + B)ᵀ are identical.

### 3. Given any square matrix $\mathbf{A}$, is $\mathbf{A} + \mathbf{A}^\top$ always symmetric? Can you prove the result by using only the result of the previous two exercises?

Yes. For matrix A:

```py
A = torch.tensor([[1, 2, 3], [4, 5, 6], [7, 8, 9]])
```

As you can see:

```py
(A + A.T) == (A + A.T).T
```

It will output:

```
tensor([[True, True, True],
        [True, True, True],
        [True, True, True]])
```

---

> 理解错题意的回答，保留在此是为了其中关于 symmetric 需要满足的条件:
> 
> No, it's not *always* symmetric. It must satisfy the following conditions:
> 
> - Square Matrix: The matrix must be square, meaning it has the same number of rows and 
> columns.
> - Equality to Transpose: The matrix must be equal to its own transpose. In other words, 
> if A is a square matrix, it is symmetric if and only if A = Aᵀ (When run `A == A.T` it 
> should return all True).
> - Diagonal Elements: The diagonal (对角线) elements of the matrix (elements along the 
> main diagonal from the top-left to the bottom-right) must remain unchanged when taking 
> the transpose. This means that `A[i][i] = Aᵀ[i][i]` for all `i`, where `i` ranges from 
> 0 to the size of the matrix minus 1.
> 
> So, for the previous two exercises, matrix A and B are not symmetric as:
> 
> ```py
> A == A.T
> ```
> 
> It will output:
> 
> ```
> tensor([[ True, False, False],
>         [False,  True, False],
>         [False, False,  True]])
> ```

### 4. We defined the tensor `X` of shape (2, 3, 4) in this section. What is the output of `len(X)`? Write your answer without implementing any code, then check your answer using code. 

The output of `len(X)` will be 2. (This answer was given after correction).

The tensor X of shape (2, 3, 4) has three dimensions: 2 along the first axis, 3 along 
the second axis, and 4 along the third axis.

In Python, when you use the `len()` function on an object, it returns the number of 
elements along the first axis (also known as the length of the object). For a tensor or 
an array, this corresponds to the size of the first dimension.

In the case of the tensor X with shape (2, 3, 4), the first axis has a size of 2. 
Therefore, len(X) will output 2.

If you were to use len() on one of the other axes, you would get different results. For 
example, `len(X[0])` would give you 3, and `len(X[0][0])` would give you 4. These 
correspond to the sizes of the second and third dimensions, respectively.

### 5. For a tensor `X` of arbitrary shape, does `len(X)` always correspond to the length of a certain axis of `X`? What is that axis?

Yes, for a tensor X of arbitrary shape, the output of `len(X)` always corresponds to the 
length of a certain axis of X. That axis is the first axis, often referred to as the 
"batch" or "sample" axis.

### 6. Run `A / A.sum(axis=1)` and see what happens. Can you analyze the reason?

```py
print("Original tensor:")
print(A)
print()

print("Summed up along dim 1 (column):")
print(A.sum(dim=1))
print()

print("The result:")
print(A / A.sum(dim=1))
```

```
Original tensor:
tensor([[1, 2, 3],
        [4, 5, 6],
        [7, 8, 9]])

Summed up along dim 1 (column):
tensor([ 6, 15, 24])

tensor([[0.1667, 0.1333, 0.1250],
        [0.6667, 0.3333, 0.2500],
        [1.1667, 0.5333, 0.3750]])
```

### 7. When traveling between two points in downtown Manhattan, what is the distance that you need to cover in terms of the coordinates, i.e., in terms of avenues and streets? Can you travel diagonally?

[WIP]

### 8. Consider a tensor with shape (2, 3, 4). What are the shapes of the summation outputs along axis 0, 1, and 2?

```py
X = torch.arange(24).reshape(2, 3, 4)

print(X, end="\n\n")

print(X.sum(dim=0).shape)
print(X.sum(dim=1).shape)
print(X.sum(dim=2).shape)
```

```
tensor([[[ 0,  1,  2,  3],
         [ 4,  5,  6,  7],
         [ 8,  9, 10, 11]],

        [[12, 13, 14, 15],
         [16, 17, 18, 19],
         [20, 21, 22, 23]]])

torch.Size([3, 4])
torch.Size([2, 4])
torch.Size([2, 3])
```

### 9. Feed a tensor with 3 or more axes to the `linalg.norm` function and observe its output. What does this function compute for tensors of arbitrary shape?

```py
import math

S = torch.arange(4).reshape(2, 2).float()  # create a simple two-dim tensor
print(torch.linalg.norm(S))  # calc norm of it
print(math.sqrt(0**2 + 1**2 + 2**2 + 3**2))  # calc norm using plain mathematics

J = torch.arange(48).reshape(2, 3, 4, 2).float()  # create a complex 4-dim tensor
print(torch.linalg.norm(J))  # calc norm of it
print(
    math.sqrt(sum(i**2 for i in range(48)))
)  # calc norm using plain mathematics which is not ideal
```

The `torch.linalg.norm` function computes the Frobenius norm by default for tensors of 
arbitrary shape. The Frobenius norm is an extension of the L2 (Euclidean) norm for 
matrices or higher dimensional tensors.

When you feed a tensor with 3 or more axes/dimensions to the torch.linalg.norm function, 
it treats the tensor **as if it were flattened** to one dimension and computes the norm 
over the entire set of values.

For instance, if we create a 3-dimensional tensor in PyTorch:

```py
import torch

T = torch.arange(24).reshape(2, 3, 4).float()
print(torch.linalg.norm(T))
```

It amounts to calculating the square root of sum of squares of all its elements (0 to 23
 in this case) treating it as a flat 1D tensor or vector.

So, irrespective of the original shape of the tensor, torch.linalg.norm() computes the 
norm in the same way - by flattening the tensor to 1D and then computing the square root
of the sum of squares of all its elements.

### 10. Define three large matrices, say $\mathbf{A} \in \mathbb{R}^{2^{10} \times 2^{16}}$, $\mathbf{B} \in \mathbb{R}^{2^{16} \times 2^{5}}$ and $\mathbf{C} \in \mathbb{R}^{2^{5} \times 2^{14}}$, for instance initialized with Gaussian random variables. You want to compute the product $\mathbf{A} \mathbf{B} \mathbf{C}$. Is there any difference in memory footprint and speed, depending on whether you compute $(\mathbf{A} \mathbf{B}) \mathbf{C}$ or $\mathbf{A} (\mathbf{B} \mathbf{C})$. Why?

> The "product" here in the title is actually dot product, not element-wise 
multiplication. 

> The number of *columns* in the first matrix (A) needs to equal to the number of 
*rows* in the second matrix (B), which is a requirement for matrix multiplication.

> The number of scalar multiplications required for multiplying two matrices can be 
calculated based on the dimensions of the matrices. For matrices A and B with shapes 
(m, n) and (n, p) respectively, performing the matrix multiplication A @ B requires 
performing m * n * p scalar multiplications. This is because, for each element in the 
resulting matrix, we compute the dot product of one row from the first matrix (A) and 
one column from the second matrix (B). Each dot product operation involves n 
multiplications (since the length of the row/column is n), and there are m * p such 
elements in the result (since the result has m rows and p columns).

```py
# Set the random seed for reproducibility
torch.manual_seed(0)

# Initialize the matrices
A = torch.randn(2**10, 2**16)
B = torch.randn(2**16, 2**5)
C = torch.randn(2**5, 2**14)

# measure the time used for running `(A @ B) @ C`
start_time = time.time()
result_1 = (A @ B) @ C
end_time = time.time()
time_taken_1 = end_time - start_time
print(f"Time taken for (A @ B) @ C: {time_taken_1} seconds")

# measure the time used for running `A @ (B @ C)`
start_time = time.time()
result_2 = A @ (B @ C)
end_time = time.time()
time_taken_2 = end_time - start_time
print(f"Time taken for A @ (B @ C): {time_taken_2} seconds")
```

Yes, there can be a significant difference in both the memory footprint and computation 
speed depending on the order of multiplication, especially with large matrices, and it's
 related to the number of operations performed in each case.

Matrix multiplication is associative (结合律), meaning that (A @ B) @ C = A @ (B @ C). 
However, the number of scalar multiplications required for each can be different.

Consider the sizes of the matrices:

- A is (2^10, 2^16)
- B is (2^16, 2^5)
- C is (2^5, 2^14)

If we compute (A @ B) @ C, then:

- First you're calculating A @ B which results in a (2^10, 2^5) matrix. This requires 
2^10 * 2^16 * 2^5 scalar multiplications.

- Then you're multiplying this result with C, requiring an additional 2^10 * 2^5 * 2^14 
scalar multiplications.

If instead you compute A @ (B @ C), then:

- First you're calculating B @ C which results in a (2^16, 2^14) matrix. This requires 
2^16 * 2^5 * 2^14 scalar multiplications.

- Then you're multiplying A with this result, requiring an additional 2^10 * 2^16 * 2^14 
scalar multiplications.

You can see that the latter requires significantly more computations. The exact 
difference will depend on the actual matrix sizes and could be much more significant 
for larger matrices. This is essentially the matrix chain multiplication problem, a 
classic dynamic programming problem.

In terms of memory usage, computing (A @ B) @ C would typically be less memory-intensive 
since you only need to keep smaller intermediates results in memory.

Therefore, the order of multiplication can have a big impact on both the memory 
footprint and computation speed in this case.

### 11. Define three large matrices, say $\mathbf{A} \in \mathbb{R}^{2^{10} \times 2^{16}}$, $\mathbf{B} \in \mathbb{R}^{2^{16} \times 2^{5}}$ and $\mathbf{C} \in \mathbb{R}^{2^{5} \times 2^{16}}$. Is there any difference in speed depending on whether you compute $\mathbf{A} \mathbf{B}$ or $\mathbf{A} \mathbf{C}^\top$? Why? What changes if you initialize $\mathbf{C} = \mathbf{B}^\top$ without cloning memory? Why?

```py
A = torch.randn((2**10, 2**16))
B = torch.randn((2**16, 2**5))
C = torch.randn((2**5, 2**16))

start_time = time.perf_counter()
result_a = A @ B  # the operation is here
time_taken = time.perf_counter() - start_time
print(f"The code took {time_taken} seconds to run.")

start_time = time.perf_counter()
result_b = A @ C.T  # the operation is here
time_taken = time.perf_counter() - start_time
print(f"The code took {time_taken} seconds to run.")
```

The outputs:

```
The code took 0.024152667028829455 seconds to run.
The code took 0.02381775004323572 seconds to run.
```

The second one (`A @ C.T`) is a little faster. The reason is:

- Logical Transpose: "Logical transpose" means that the data structure that is 
representing the matrix in the memory of your computer knows that it's supposed to act 
as if it has been transposed, but the actual arrangement of data in memory hasn't 
changed. So if the original matrix was 3x2 (3 rows and 2 columns), the transposed matrix
is logically 2x3 but physically still stored as if it was 3x2.

- Accessing Elements: Computer memory is arranged linearly, and accessing elements that 
are close together in memory is faster than accessing elements that are spaced apart. 
Matrices in Python (via numpy or PyTorch) are typically stored in a "row-major" format, 
which means the entire first row of the matrix is stored together in memory, then the 
second row, and so on. This makes accessing all the elements in a row faster than 
accessing all the elements in a column.

- Transpose and Matrix Multiplication: When you perform a matrix multiplication, the 
rows of the first matrix are combined with the columns of the second. This means the 
second matrix has to access its data "column by column," which isn't very efficient with 
a row-major stored matrix, because the data isn't contiguous in memory.

However, when you take a "logical transpose," you're effectively telling the program to 
treat the rows of your matrix as if they were columns. This makes the memory access for 
the second matrix in your multiplication much more efficient, because what's physically 
stored as a row (and is therefore contiguous in memory) is being accessed as if it was 
a column.

Performance Benefit: As such, even though $\mathbf{A}\mathbf{B}$ and 
$\mathbf{A}\mathbf{C}^T$ theoretically involve the same number of operations (given that
 the dimensions of B and $\mathbf{C}^\top$ are the same), the latter could potentially 
 be faster due to this more efficient memory access pattern.

### 12. Define three matrices, say $\mathbf{A}, \mathbf{B}, \mathbf{C} \in \mathbb{R}^{100 \times 200}$. Constitute a tensor with 3 axes by stacking $[\mathbf{A}, \mathbf{B}, \mathbf{C}]$. What is the dimensionality? Slice out the second coordinate of the third axis to recover $\mathbf{B}$. Check that your answer is correct.

```py
A = torch.randn(100, 200)
B = torch.randn(100, 200)
C = torch.randn(100, 200)

# stack A, B and C
T = torch.stack((A, B, C))

# slice out to recover B, as you can see they are equal.
B == T[1]
```

```py
tensor([[True, True, True,  ..., True, True, True],
        [True, True, True,  ..., True, True, True],
        [True, True, True,  ..., True, True, True],
        ...,
        [True, True, True,  ..., True, True, True],
        [True, True, True,  ..., True, True, True],
        [True, True, True,  ..., True, True, True]])
```

The dimensionality is 3 (by running `len(stacked)`).